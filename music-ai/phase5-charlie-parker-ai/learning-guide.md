# Phase 5: Charlie Parker AI í”„ë¡œì íŠ¸
## BirdAI - ì‹¤ì „ Charlie Parker ìŠ¤íƒ€ì¼ AI ê°œë°œ (3ê°œì›”)

---

## ğŸ¯ ìµœì¢… ëª©í‘œ

**"BirdAI"**: ì¬ì¦ˆ ë®¤ì§€ì…˜ì´ Charlie Parkerë¡œ ì¸ì •í•  ìˆ˜ ìˆëŠ” ì¦‰í¥ ì—°ì£¼ AI ì‹œìŠ¤í…œ êµ¬ì¶•

### ì„±ê³µ ê¸°ì¤€
- âœ… ë¸”ë¼ì¸ë“œ í…ŒìŠ¤íŠ¸ 50% ì´ìƒ í†µê³¼
- âœ… ì½”ë“œ ì§„í–‰ ê¸°ë°˜ ì¦‰í¥ì—°ì£¼ ê°€ëŠ¥
- âœ… 5ë¶„ ì´ìƒ ì—°ì† ìƒì„± (ë°˜ë³µ ì—†ì´)
- âœ… Be-bop íŠ¹ì§• 5ê°€ì§€ ì´ìƒ êµ¬í˜„
- âœ… ì‹¤ì‹œê°„ ì¸í„°ë™ì…˜ ê°€ëŠ¥

---

## ğŸ—ºï¸ BirdAI ê°œë°œ ë¡œë“œë§µ

### Version 1.0 (Week 1-4): ê¸°ë³¸ ë©œë¡œë”” ìƒì„±
**ëª©í‘œ**: ë‹¨ìˆœ MIDI ìƒì„±

```python
# ê¸°ëŠ¥:
- Music Transformer ê¸°ë°˜
- ë¬´ì¡°ê±´ ë©œë¡œë”” ìƒì„± (ì½”ë“œ ë¬´ê´€)
- 32-bar ê³ ì • ê¸¸ì´

# í•™ìŠµ:
- Charlie Parker MIDI 100ê°œ
- ë°ì´í„° ì¦ê°• â†’ 1000ê°œ
- Epoch: 50

# ê²°ê³¼:
input: [ì‹œì‘ ìŒí‘œ]
output: 32-bar ì¬ì¦ˆ ì†”ë¡œ (MIDI)
```

---

### Version 2.0 (Week 5-8): ì¡°ê±´ë¶€ ìƒì„±
**ëª©í‘œ**: ì½”ë“œ ì§„í–‰ ë”°ë¼ ì¦‰í¥ì—°ì£¼

```python
# ê¸°ëŠ¥:
- Conditional generation
- ì…ë ¥: ì½”ë“œ ì§„í–‰ (ì˜ˆ: Dm7 G7 Cmaj7)
- ì¶œë ¥: ì½”ë“œì— ë§ëŠ” ì†”ë¡œ

# êµ¬í˜„:
class ConditionalTransformer(nn.Module):
    def __init__(self):
        self.chord_embedding = nn.Embedding(200, 512)  # ì½”ë“œ ì„ë² ë”©
        self.note_embedding = nn.Embedding(128, 512)   # ìŒí‘œ ì„ë² ë”©

    def forward(self, notes, chords):
        # ì½”ë“œ + ìŒí‘œ ì„ë² ë”© ê²°í•©
        chord_emb = self.chord_embedding(chords)
        note_emb = self.note_embedding(notes)
        combined = chord_emb + note_emb
        # Transformer...

# í•™ìŠµ:
- ì½”ë“œ ì§„í–‰ ìë™ ì¶”ì¶œ (music21)
- <chord, melody> ìŒìœ¼ë¡œ í•™ìŠµ

# í‰ê°€:
- ii-V-I ì§„í–‰ì— ì ì ˆí•œ ìŒ ì‚¬ìš©í•˜ëŠ”ê°€?
- ì½”ë“œ í†¤ ë¹„ìœ¨ 65% ì´ìƒ
```

---

### Version 3.0 (Week 9-10): ì‹¤ì‹œê°„ ì¸í„°ë™ì…˜
**ëª©í‘œ**: Call & Response

```python
# ê¸°ëŠ¥:
- ì‚¬ìš©ìê°€ 4-bar ë©œë¡œë”” ì…ë ¥
- AIê°€ ì‘ë‹µ 4-bar ìƒì„±
- ì¬ì¦ˆ ì¼ ì„¸ì…˜!

# êµ¬í˜„:
def call_and_response(user_input_midi):
    # 1. ì…ë ¥ ë¶„ì„
    motif = extract_motif(user_input_midi)

    # 2. ìœ ì‚¬ ëª¨í‹°í”„ ë³€í˜•
    response = model.generate(
        context=user_input_midi,
        motif_constraint=motif,  # ëª¨í‹°í”„ ì¬í™œìš©
        variation=True
    )

    return response

# í‰ê°€:
- ì…ë ¥ê³¼ ìŒì•…ì  ì—°ê²°ì„±
- ë‹¤ì–‘ì„± (ë‹¨ìˆœ ë°˜ë³µ ì•„ë‹˜)
```

---

### Version 4.0 (Week 11-12): ìŠ¤íƒ€ì¼ ì¡°ì ˆ
**ëª©í‘œ**: Parker-ness ì¡°ì ˆ ê°€ëŠ¥

```python
# ê¸°ëŠ¥:
- Style intensity slider (0-100)
- 0: ë³´ìˆ˜ì  (ì½”ë“œ í†¤ ìœ„ì£¼)
- 100: ë§¤ìš° íŒŒì»¤ìŠ¤ëŸ¬ì›€ (Chromatic, ë¹ ë¦„)

# êµ¬í˜„:
# Conditional LayerNorm (FiLM)
class StyleConditionalLayer(nn.Module):
    def __init__(self):
        self.style_fc = nn.Linear(1, 512*2)  # gamma, beta

    def forward(self, x, style_intensity):
        gamma, beta = self.style_fc(style_intensity).chunk(2, dim=-1)
        return gamma * x + beta

# ë˜ëŠ” Classifier-Free Guidance
output = model(chords, style=None)  # Unconditional
output_styled = model(chords, style="parker")  # Conditional

final = (1 - guidance) * output + guidance * output_styled

# í‰ê°€:
- Style 0: ì•ˆì „í•œ ì¬ì¦ˆ
- Style 50: ê· í˜•ì¡íŒ Parker
- Style 100: ë§¤ìš° ì‹¤í—˜ì 
```

---

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
bird-ai/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ charlie_parker/          # 100+ MIDI
â”‚   â”œâ”€â”€ processed/               # ì „ì²˜ë¦¬ ì™„ë£Œ
â”‚   â””â”€â”€ augmented/               # ë°ì´í„° ì¦ê°•
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ music_transformer.py     # Core model
â”‚   â”œâ”€â”€ conditional_transformer.py
â”‚   â”œâ”€â”€ style_controller.py
â”‚   â””â”€â”€ vae_latent.py
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train.py                 # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ dataset.py               # PyTorch Dataset
â”‚   â”œâ”€â”€ config.yaml              # í•˜ì´í¼íŒŒë¼ë¯¸í„°
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ metrics.py               # ì •ëŸ‰ í‰ê°€
â”‚   â”œâ”€â”€ blind_test.py            # ë¸”ë¼ì¸ë“œ í…ŒìŠ¤íŠ¸
â”‚   â””â”€â”€ parker_score.py          # Parker-ness ì ìˆ˜
â”‚
â”œâ”€â”€ generation/
â”‚   â”œâ”€â”€ generate.py              # MIDI ìƒì„±
â”‚   â”œâ”€â”€ interactive.py           # ì‹¤ì‹œê°„ ì¸í„°ë™ì…˜
â”‚   â””â”€â”€ chord_following.py
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_model_experiments.ipynb
â”‚   â””â”€â”€ 03_evaluation.ipynb
â”‚
â””â”€â”€ README.md
```

---

## ğŸ§  í•µì‹¬ ê¸°ìˆ  êµ¬í˜„

### 1. Be-bop íŠ¹ì§• ê°•í™”

```python
# Chromatic Approach ê°•ì œ
class BebopLayer(nn.Module):
    def __init__(self):
        self.chromatic_attention = ChromaticAttention()

    def forward(self, x, target_note):
        # target_note ì§ì „ì— chromatic approach ì„ í˜¸
        approach_logits = self.chromatic_attention(x, target_note)
        return approach_logits

# í•™ìŠµ ì‹œ Reward
if is_chromatic_approach(pred_note, target_note):
    reward += 1.0

loss = cross_entropy_loss - reward_weight * reward
```

### 2. ì½”ë“œ í†¤ ë¹„ìœ¨ ìœ ì§€

```python
def chord_tone_loss(predicted_notes, chord):
    """
    ì½”ë“œ í†¤ ë¹„ìœ¨ 65% ìœ ì§€

    Args:
        predicted_notes: (batch, seq_len, vocab)
        chord: (batch, seq_len, chord_dim)
    """
    chord_tones = get_chord_tones(chord)  # [0, 4, 7] for C major

    # Predicted notesê°€ chord toneì¸ì§€
    is_chord_tone = is_in(predicted_notes, chord_tones)

    # ëª©í‘œ: 65%
    actual_ratio = is_chord_tone.mean()
    target_ratio = 0.65

    loss = (actual_ratio - target_ratio) ** 2
    return loss

# Total loss
loss = ce_loss + 0.1 * chord_tone_loss + 0.05 * rhythm_loss
```

### 3. ë¦¬ë“¬ ë‹¤ì–‘ì„±

```python
class RhythmGenerator(nn.Module):
    """
    ìŒí‘œ + ë¦¬ë“¬ ë™ì‹œ ìƒì„±

    Output: [(pitch, duration), ...]
    """
    def __init__(self):
        self.pitch_head = nn.Linear(512, 128)     # Pitch
        self.duration_head = nn.Linear(512, 32)   # Duration (quantized)

    def forward(self, x):
        pitch_logits = self.pitch_head(x)
        duration_logits = self.duration_head(x)

        return pitch_logits, duration_logits

# ì¬ì¦ˆ ë¦¬ë“¬: 8ë¶„ìŒí‘œ 70%, 16ë¶„ìŒí‘œ 15%, ...
rhythm_prior = [0.7, 0.15, 0.05, 0.05, 0.05]  # 8th, 16th, quarter, ...

# KL divergenceë¡œ prior ë”°ë¥´ê²Œ
rhythm_kl_loss = KL(predicted_rhythm, rhythm_prior)
```

---

## ğŸ“Š í‰ê°€ ì‹œìŠ¤í…œ

### Parker-ness Score (0-100)

```python
def calculate_parker_score(generated_midi):
    """
    Charlie Parker ìœ ì‚¬ë„ ì ìˆ˜

    Returns:
        score: 0-100
        breakdown: dict of subscores
    """
    score = 0
    breakdown = {}

    # 1. ìŒì—­ (10ì )
    pitch_range = get_pitch_range(generated_midi)
    if 53 <= pitch_range[0] and pitch_range[1] <= 84:  # F3-C6
        score += 10
    breakdown['pitch_range'] = 10

    # 2. ì½”ë“œ í†¤ ë¹„ìœ¨ (20ì )
    chord_tone_ratio = calculate_chord_tone_ratio(generated_midi)
    if 0.60 <= chord_tone_ratio <= 0.70:
        score += 20
    elif 0.55 <= chord_tone_ratio <= 0.75:
        score += 15
    breakdown['chord_tone'] = 20

    # 3. Chromatic approach (15ì )
    chromatic_count = count_chromatic_approaches(generated_midi)
    if chromatic_count >= 10:
        score += 15
    breakdown['chromatic'] = 15

    # 4. Bebop scale ì‚¬ìš© (15ì )
    bebop_usage = calculate_bebop_scale_usage(generated_midi)
    if bebop_usage >= 0.5:
        score += 15
    breakdown['bebop'] = 15

    # 5. ë¦¬ë“¬ ë‹¤ì–‘ì„± (10ì )
    rhythm_entropy = calculate_rhythm_entropy(generated_midi)
    if rhythm_entropy >= 1.5:
        score += 10
    breakdown['rhythm'] = 10

    # 6. í”„ë ˆì´ì¦ˆ ê¸¸ì´ (10ì )
    phrase_lengths = detect_phrases(generated_midi)
    avg_phrase = np.mean(phrase_lengths)
    if 2 <= avg_phrase <= 4:  # 2-4 bar
        score += 10
    breakdown['phrase'] = 10

    # 7. ìŒì • ê°„ê²© ë¶„í¬ (10ì )
    interval_dist = calculate_interval_distribution(generated_midi)
    parker_dist = load_parker_interval_distribution()
    similarity = cosine_similarity(interval_dist, parker_dist)
    score += int(similarity * 10)
    breakdown['interval'] = 10

    # 8. Velocity ë‹¤ì–‘ì„± (10ì )
    velocity_std = np.std([note.velocity for note in generated_midi.notes])
    if 15 <= velocity_std <= 25:
        score += 10
    breakdown['velocity'] = 10

    return score, breakdown

# ì‚¬ìš©:
score, details = calculate_parker_score(generated_midi)
print(f"Parker-ness Score: {score}/100")
print(f"Details: {details}")

# ëª©í‘œ: 70/100 ì´ìƒ
```

### ë¸”ë¼ì¸ë“œ í…ŒìŠ¤íŠ¸

```python
# blind_test.py
import random

def blind_test(real_midis, generated_midis, num_testers=10):
    """
    ì¬ì¦ˆ ë®¤ì§€ì…˜ì—ê²Œ ë¸”ë¼ì¸ë“œ í…ŒìŠ¤íŠ¸

    Args:
        real_midis: Charlie Parker ì§„ì§œ ì†”ë¡œ
        generated_midis: BirdAI ìƒì„± ì†”ë¡œ
        num_testers: í…ŒìŠ¤í„° ìˆ˜

    Returns:
        success_rate: AI ì†”ë¡œê°€ ì§„ì§œë¡œ ì¸ì •ë°›ì€ ë¹„ìœ¨
    """
    # 50% Real, 50% Generated
    test_set = random.sample(real_midis, 10) + random.sample(generated_midis, 10)
    random.shuffle(test_set)

    # í…ŒìŠ¤í„°ì—ê²Œ
    results = []
    for tester in range(num_testers):
        print(f"\nTester {tester + 1}:")
        correct = 0
        for idx, midi in enumerate(test_set):
            # MIDI ì¬ìƒ
            play_midi(midi)

            # í‰ê°€
            answer = input(f"#{idx + 1}: Is this Charlie Parker? (y/n): ")
            is_real = midi in real_midis

            if (answer == 'y' and is_real) or (answer == 'n' and not is_real):
                correct += 1

        accuracy = correct / len(test_set)
        results.append(accuracy)

    avg_accuracy = np.mean(results)
    print(f"\nAverage Accuracy: {avg_accuracy:.2%}")

    # AI ì†”ë¡œê°€ ì§„ì§œë¡œ ì¸ì •ë°›ì€ ë¹„ìœ¨
    fooled_rate = 1 - avg_accuracy  # í‹€ë¦° ë¹„ìœ¨
    print(f"AI Fooled Rate: {fooled_rate:.2%}")

    return fooled_rate

# ëª©í‘œ: 50% ì´ìƒ (random guess)
```

---

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: ëª¨ë¸ì´ ë°˜ë³µë§Œ í•¨
**ì›ì¸**: ê³¼ì í•©, ë°ì´í„° ë¶€ì¡±

**í•´ê²°ì±…**:
```python
# 1. Dropout ì¦ê°€
model = MusicTransformer(dropout=0.3)  # 0.1 â†’ 0.3

# 2. ë°ì´í„° ì¦ê°• ë” aggressive
augment_factor = 20  # 10 â†’ 20

# 3. Nucleus sampling
generated = model.generate(top_p=0.9)  # Top-k ëŒ€ì‹ 
```

### ë¬¸ì œ 2: ì½”ë“œ ì•ˆ ë”°ë¦„
**ì›ì¸**: Conditioning ì•½í•¨

**í•´ê²°ì±…**:
```python
# 1. Chord loss weight ì¦ê°€
loss = ce_loss + 0.5 * chord_loss  # 0.1 â†’ 0.5

# 2. Chord embedding ê°•í™”
self.chord_embedding = nn.Embedding(200, 1024)  # 512 â†’ 1024
```

### ë¬¸ì œ 3: ìŒì•…ì ìœ¼ë¡œ ì´ìƒí•¨
**ì›ì¸**: ìŒì•… ì´ë¡  ì œì•½ ì—†ìŒ

**í•´ê²°ì±…**:
```python
# Constrained decoding
def is_valid_note(prev_note, next_note, chord):
    # 1. ìŒì—­ ì œí•œ
    if next_note < 53 or next_note > 84:
        return False

    # 2. í° ë„ì•½ ì œí•œ (ì˜¥íƒ€ë¸Œ ì´ìƒ)
    if abs(next_note - prev_note) > 12:
        return False

    # 3. ì½”ë“œ ì™¸ìŒì€ passing noteë§Œ
    if next_note not in get_scale(chord):
        # ì´ì „/ë‹¤ìŒì´ ì½”ë“œ í†¤ì´ì–´ì•¼
        return is_passing_note(prev_note, next_note, next_next_note)

    return True

# ìƒì„± ì‹œ filtering
logits[~is_valid_note(prev, next, chord)] = -inf
```

---

## ğŸ¯ ë§ˆì¼ìŠ¤í†¤

- [ ] **Week 4**: BirdAI v1.0 - ê¸°ë³¸ ìƒì„± ì„±ê³µ
- [ ] **Week 8**: BirdAI v2.0 - ì½”ë“œ ì§„í–‰ ë”°ë¦„
- [ ] **Week 10**: BirdAI v3.0 - ì‹¤ì‹œê°„ ì¸í„°ë™ì…˜
- [ ] **Week 12**: BirdAI v4.0 - ìŠ¤íƒ€ì¼ ì¡°ì ˆ ê°€ëŠ¥
- [ ] **Finale**: Parker-ness Score 70+ ë‹¬ì„±
- [ ] **Finale**: ë¸”ë¼ì¸ë“œ í…ŒìŠ¤íŠ¸ 50%+ í†µê³¼

---

## ğŸ”— ë‹¤ìŒ ë‹¨ê³„

Phase 5 ì™„ë£Œ ì‹œ:
- âœ… **BirdAI v4.0**: ì™„ì„±ëœ Charlie Parker AI
- âœ… **í¬íŠ¸í´ë¦¬ì˜¤**: GitHub showcase í”„ë¡œì íŠ¸
- âœ… **ë…¼ë¬¸/ë¸”ë¡œê·¸**: ê¸°ìˆ  ë¬¸ì„œí™”
- âœ… **ë°ëª¨**: ì›¹ ì¸í„°í˜ì´ìŠ¤ ì¤€ë¹„

**â¡ï¸ [Phase 6: Deployment & Portfolio](../phase6-deployment/learning-guide.md)**

ì´ì œ ì„¸ìƒì— ê³µê°œí•  ì°¨ë¡€!

---

**"Bird lives through AI. Charlie Parkerì˜ ì°½ì˜ì„±ì„ ì½”ë“œë¡œ ì˜ì›íˆ."**

*Estimated Time: 90ì¼ (í•˜ë£¨ 4-5ì‹œê°„)*
*Difficulty: â­â­â­â­â­*
*Next: Phase 6 - Deployment* ğŸš€

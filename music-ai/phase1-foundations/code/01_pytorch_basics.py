"""
Phase 1 - PyTorch Basics
Tensor ì—°ì‚°, Autograd, GPU ì‚¬ìš©ë²• ë§ˆìŠ¤í„°
"""

import torch
import torch.nn as nn
import numpy as np
import time

def tensor_operations():
    """ê¸°ë³¸ Tensor ì—°ì‚° ì—°ìŠµ"""
    print("="*50)
    print("1. Tensor ìƒì„±")
    print("="*50)

    # ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ Tensor ìƒì„±
    a = torch.tensor([1, 2, 3, 4, 5])
    b = torch.zeros(3, 4)
    c = torch.ones(2, 3, 4)
    d = torch.randn(2, 3)  # ì •ê·œë¶„í¬
    e = torch.arange(0, 10, 2)

    print(f"1D Tensor: {a}")
    print(f"Zeros (3x4): \n{b}")
    print(f"Random (2x3): \n{d}")
    print(f"Range: {e}")

    # Tensor íƒ€ì… ë³€í™˜
    f = torch.tensor([1, 2, 3], dtype=torch.float32)
    g = f.long()  # int64ë¡œ ë³€í™˜
    print(f"\nFloat32: {f.dtype}, Long: {g.dtype}")

    print("\n" + "="*50)
    print("2. Tensor ì—°ì‚°")
    print("="*50)

    x = torch.randn(3, 4)
    y = torch.randn(3, 4)

    # ê¸°ë³¸ ì—°ì‚°
    z1 = x + y
    z2 = torch.add(x, y)
    z3 = x * y  # element-wise
    z4 = torch.matmul(x, y.T)  # matrix multiplication

    print(f"Addition: {z1.shape}")
    print(f"Element-wise multiply: {z3.shape}")
    print(f"Matrix multiply: {z4.shape}")

    # Broadcasting
    a = torch.ones(3, 1)
    b = torch.ones(1, 4)
    c = a + b  # (3, 1) + (1, 4) = (3, 4)
    print(f"\nBroadcasting: {a.shape} + {b.shape} = {c.shape}")

    # Reshaping
    original = torch.randn(2, 3, 4)
    reshaped = original.view(2, 12)  # viewëŠ” ë©”ëª¨ë¦¬ ê³µìœ 
    reshaped_new = original.reshape(6, 4)  # reshapeëŠ” ë³µì‚¬ ê°€ëŠ¥

    print(f"\nReshape: {original.shape} -> {reshaped.shape}")


def autograd_basics():
    """ìë™ ë¯¸ë¶„ (Autograd) ì´í•´"""
    print("\n" + "="*50)
    print("3. Autograd - ìë™ ë¯¸ë¶„")
    print("="*50)

    # requires_grad=Trueë¡œ gradient ì¶”ì  ì‹œì‘
    x = torch.tensor([2.0], requires_grad=True)
    y = x ** 2 + 3 * x + 1

    print(f"x = {x.item()}")
    print(f"y = x^2 + 3x + 1 = {y.item()}")

    # ì—­ì „íŒŒ
    y.backward()

    # dy/dx = 2x + 3 = 2*2 + 3 = 7
    print(f"dy/dx = {x.grad.item()} (ì˜ˆìƒ: 7.0)")

    # ë‹¤ë³€ìˆ˜ í•¨ìˆ˜
    a = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
    b = torch.tensor([4.0, 5.0, 6.0], requires_grad=True)

    c = a + b
    d = c.sum()

    d.backward()

    print(f"\na = {a.data}")
    print(f"b = {b.data}")
    print(f"d = sum(a + b) = {d.item()}")
    print(f"âˆ‚d/âˆ‚a = {a.grad}")
    print(f"âˆ‚d/âˆ‚b = {b.grad}")

    # Gradient ëˆ„ì  ë°©ì§€
    a.grad.zero_()
    b.grad.zero_()
    print("\nGradient cleared!")


def neural_network_basics():
    """ê°„ë‹¨í•œ ì‹ ê²½ë§ êµ¬í˜„"""
    print("\n" + "="*50)
    print("4. Simple Neural Network")
    print("="*50)

    class SimpleNet(nn.Module):
        def __init__(self, input_size, hidden_size, output_size):
            super(SimpleNet, self).__init__()
            self.fc1 = nn.Linear(input_size, hidden_size)
            self.relu = nn.ReLU()
            self.fc2 = nn.Linear(hidden_size, output_size)

        def forward(self, x):
            x = self.fc1(x)
            x = self.relu(x)
            x = self.fc2(x)
            return x

    # ëª¨ë¸ ìƒì„±
    model = SimpleNet(10, 20, 5)
    print(model)

    # íŒŒë¼ë¯¸í„° í™•ì¸
    total_params = sum(p.numel() for p in model.parameters())
    print(f"\nTotal parameters: {total_params:,}")

    # Forward pass
    x = torch.randn(32, 10)  # batch_size=32, input_size=10
    output = model(x)
    print(f"\nInput shape: {x.shape}")
    print(f"Output shape: {output.shape}")

    # íŠ¹ì • ë ˆì´ì–´ íŒŒë¼ë¯¸í„° ì ‘ê·¼
    print(f"\nFirst layer weight shape: {model.fc1.weight.shape}")
    print(f"First layer bias shape: {model.fc1.bias.shape}")


def gpu_usage():
    """GPU ì‚¬ìš©ë²• ë° ì†ë„ ë¹„êµ"""
    print("\n" + "="*50)
    print("5. GPU Usage")
    print("="*50)

    # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    if torch.cuda.is_available():
        print(f"GPU Name: {torch.cuda.get_device_name(0)}")
        print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

    # CPU vs GPU ì†ë„ ë¹„êµ
    size = 5000

    # CPU
    x_cpu = torch.randn(size, size)
    y_cpu = torch.randn(size, size)

    start = time.time()
    z_cpu = torch.matmul(x_cpu, y_cpu)
    cpu_time = time.time() - start

    print(f"\nCPU time: {cpu_time:.4f} seconds")

    # GPU
    if torch.cuda.is_available():
        x_gpu = x_cpu.to(device)
        y_gpu = y_cpu.to(device)

        # Warm-up (GPU ì´ˆê¸°í™” ì‹œê°„ ì œì™¸)
        _ = torch.matmul(x_gpu, y_gpu)
        torch.cuda.synchronize()

        start = time.time()
        z_gpu = torch.matmul(x_gpu, y_gpu)
        torch.cuda.synchronize()  # GPU ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
        gpu_time = time.time() - start

        print(f"GPU time: {gpu_time:.4f} seconds")
        print(f"Speedup: {cpu_time / gpu_time:.2f}x")

        # GPU ë©”ëª¨ë¦¬ í™•ì¸
        print(f"\nGPU Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
        print(f"GPU Memory cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB")

        # ë©”ëª¨ë¦¬ í•´ì œ
        del x_gpu, y_gpu, z_gpu
        torch.cuda.empty_cache()
        print("GPU memory cleared!")


def practical_tips():
    """ì‹¤ì „ íŒ"""
    print("\n" + "="*50)
    print("6. Practical Tips")
    print("="*50)

    # 1. Tensorì™€ NumPy ë³€í™˜
    np_array = np.array([1, 2, 3, 4, 5])
    tensor = torch.from_numpy(np_array)
    back_to_np = tensor.numpy()

    print("1. NumPy â†” Tensor ë³€í™˜")
    print(f"   NumPy: {np_array}")
    print(f"   Tensor: {tensor}")

    # 2. no_grad() - Gradient ê³„ì‚° ì•ˆ í•¨ (ì¶”ë¡  ì‹œ)
    x = torch.randn(10, requires_grad=True)

    with torch.no_grad():
        y = x * 2

    print(f"\n2. no_grad() context")
    print(f"   y.requires_grad = {y.requires_grad}")  # False

    # 3. detach() - Gradient graphì—ì„œ ë¶„ë¦¬
    x = torch.randn(10, requires_grad=True)
    y = x.detach()

    print(f"\n3. detach()")
    print(f"   y.requires_grad = {y.requires_grad}")  # False

    # 4. in-place ì—°ì‚° (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    x = torch.randn(3, 3)
    print(f"\n4. In-place operations")
    print(f"   Before: {id(x)}")
    x.add_(1)  # x = x + 1 (in-place)
    print(f"   After: {id(x)}")  # ê°™ì€ ë©”ëª¨ë¦¬ ì£¼ì†Œ

    # 5. ëª¨ë¸ ì €ì¥/ë¡œë“œ
    model = nn.Linear(10, 5)

    # ì €ì¥
    torch.save(model.state_dict(), "model.pth")
    print(f"\n5. Model saved to model.pth")

    # ë¡œë“œ
    model_new = nn.Linear(10, 5)
    model_new.load_state_dict(torch.load("model.pth"))
    print(f"   Model loaded successfully!")

    # 6. ëœë¤ ì‹œë“œ ê³ ì • (ì¬í˜„ì„±)
    torch.manual_seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(42)

    print(f"\n6. Random seed fixed for reproducibility")


def main():
    """ëª¨ë“  ì˜ˆì œ ì‹¤í–‰"""
    print("\n" + "ğŸ”¥"*25)
    print(" "*20 + "PyTorch Basics Tutorial")
    print("ğŸ”¥"*25 + "\n")

    tensor_operations()
    autograd_basics()
    neural_network_basics()
    gpu_usage()
    practical_tips()

    print("\n" + "="*50)
    print("âœ… All examples completed!")
    print("="*50)
    print("\nğŸ’¡ Next Steps:")
    print("   1. Modify the code and experiment")
    print("   2. Try larger tensor sizes")
    print("   3. Move to 02_neural_network.py")
    print("\n")


if __name__ == "__main__":
    main()

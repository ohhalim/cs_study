"""
Phase 1 - Neural Network Training
ì™„ì „í•œ í•™ìŠµ íŒŒì´í”„ë¼ì¸: DataLoader, Loss, Optimizer, Training Loop
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torch.utils.tensorboard import SummaryWriter
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm


class MusicPredictor(nn.Module):
    """ê°„ë‹¨í•œ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  (MLP) - ìŒì•… íŒ¨í„´ í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜"""

    def __init__(self, input_size=88, hidden_sizes=[256, 128, 64], output_size=88):
        """
        Args:
            input_size: ì…ë ¥ ì°¨ì› (ì˜ˆ: 88ê°œ í”¼ì•„ë…¸ ê±´ë°˜)
            hidden_sizes: ì€ë‹‰ì¸µ í¬ê¸° ë¦¬ìŠ¤íŠ¸
            output_size: ì¶œë ¥ ì°¨ì› (ë‹¤ìŒ ìŒí‘œ ì˜ˆì¸¡)
        """
        super(MusicPredictor, self).__init__()

        layers = []
        prev_size = input_size

        for hidden_size in hidden_sizes:
            layers.append(nn.Linear(prev_size, hidden_size))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(0.2))  # ê³¼ì í•© ë°©ì§€
            prev_size = hidden_size

        layers.append(nn.Linear(prev_size, output_size))

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)


def create_dummy_music_data(num_samples=10000, input_size=88, output_size=88):
    """
    ë”ë¯¸ ìŒì•… ë°ì´í„° ìƒì„± (ì‹¤ì œë¡œëŠ” MIDIì—ì„œ ì¶”ì¶œ)

    ì‹œë®¬ë ˆì´ì…˜: ì…ë ¥ ìŒí‘œ â†’ ë‹¤ìŒ ìŒí‘œ ì˜ˆì¸¡
    """
    # ìŒì•…ì  íŒ¨í„´ ì‹œë®¬ë ˆì´ì…˜ (ì˜ˆ: C major scale í¸í–¥)
    X = torch.randn(num_samples, input_size)

    # ê°„ë‹¨í•œ ê·œì¹™: ì…ë ¥ì˜ weighted sum + noise
    weights = torch.randn(input_size, output_size) * 0.1
    y = torch.matmul(X, weights) + torch.randn(num_samples, output_size) * 0.5

    # í™œì„±í™” í™•ë¥ ë¡œ ë³€í™˜ (0-1)
    y = torch.sigmoid(y)

    return X, y


def train_epoch(model, dataloader, criterion, optimizer, device):
    """1 ì—í­ í•™ìŠµ"""
    model.train()
    total_loss = 0

    for batch_X, batch_y in dataloader:
        batch_X, batch_y = batch_X.to(device), batch_y.to(device)

        # Forward pass
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    return total_loss / len(dataloader)


def validate(model, dataloader, criterion, device):
    """ê²€ì¦"""
    model.eval()
    total_loss = 0

    with torch.no_grad():
        for batch_X, batch_y in dataloader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            total_loss += loss.item()

    return total_loss / len(dataloader)


def plot_training_history(train_losses, val_losses):
    """í•™ìŠµ ê³¡ì„  ì‹œê°í™”"""
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='Train Loss', linewidth=2)
    plt.plot(val_losses, label='Validation Loss', linewidth=2)
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Loss', fontsize=12)
    plt.title('Training History', fontsize=14, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('training_history.png', dpi=150)
    print("ğŸ“Š Training curve saved to training_history.png")


def main():
    """ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸"""

    print("\n" + "ğŸµ"*25)
    print(" "*15 + "Neural Network Training Pipeline")
    print("ğŸµ"*25 + "\n")

    # ==================== ì„¤ì • ====================
    print("âš™ï¸  Configuration")
    print("="*50)

    # í•˜ì´í¼íŒŒë¼ë¯¸í„°
    config = {
        'input_size': 88,
        'hidden_sizes': [256, 128, 64],
        'output_size': 88,
        'num_epochs': 50,
        'batch_size': 64,
        'learning_rate': 0.001,
        'train_split': 0.8,
    }

    for key, value in config.items():
        print(f"   {key}: {value}")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"   device: {device}")
    print()

    # ==================== ë°ì´í„° ì¤€ë¹„ ====================
    print("ğŸ“¦ Preparing Data")
    print("="*50)

    X, y = create_dummy_music_data(num_samples=10000)
    print(f"   Dataset: {X.shape[0]} samples")
    print(f"   Input shape: {X.shape[1]}")
    print(f"   Output shape: {y.shape[1]}")

    # Train/Validation split
    train_size = int(config['train_split'] * len(X))
    X_train, X_val = X[:train_size], X[train_size:]
    y_train, y_val = y[:train_size], y[train_size:]

    print(f"   Train: {len(X_train)} samples")
    print(f"   Validation: {len(X_val)} samples")

    # DataLoader ìƒì„±
    train_dataset = TensorDataset(X_train, y_train)
    val_dataset = TensorDataset(X_val, y_val)

    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'],
                            shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'],
                          shuffle=False, num_workers=0)

    print(f"   Batches per epoch: {len(train_loader)}")
    print()

    # ==================== ëª¨ë¸ ìƒì„± ====================
    print("ğŸ§  Building Model")
    print("="*50)

    model = MusicPredictor(
        input_size=config['input_size'],
        hidden_sizes=config['hidden_sizes'],
        output_size=config['output_size']
    ).to(device)

    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(model)
    print(f"\n   Total parameters: {total_params:,}")
    print(f"   Trainable parameters: {trainable_params:,}")
    print()

    # ==================== Loss & Optimizer ====================
    print("ğŸ¯ Loss Function & Optimizer")
    print("="*50)

    criterion = nn.MSELoss()  # íšŒê·€ ë¬¸ì œ
    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=5, verbose=True
    )

    print(f"   Criterion: {criterion}")
    print(f"   Optimizer: Adam (lr={config['learning_rate']})")
    print(f"   Scheduler: ReduceLROnPlateau")
    print()

    # ==================== í•™ìŠµ ====================
    print("ğŸš€ Training")
    print("="*50)

    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    patience_counter = 0
    early_stop_patience = 10

    # TensorBoard (ì„ íƒ)
    writer = SummaryWriter('runs/music_predictor')

    for epoch in range(config['num_epochs']):
        # í•™ìŠµ
        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)
        train_losses.append(train_loss)

        # ê²€ì¦
        val_loss = validate(model, val_loader, criterion, device)
        val_losses.append(val_loss)

        # Learning rate scheduling
        scheduler.step(val_loss)

        # ë¡œê·¸
        print(f"Epoch [{epoch+1:3d}/{config['num_epochs']}] | "
              f"Train Loss: {train_loss:.4f} | "
              f"Val Loss: {val_loss:.4f} | "
              f"LR: {optimizer.param_groups[0]['lr']:.6f}")

        # TensorBoard ê¸°ë¡
        writer.add_scalar('Loss/train', train_loss, epoch)
        writer.add_scalar('Loss/val', val_loss, epoch)
        writer.add_scalar('Learning_Rate', optimizer.param_groups[0]['lr'], epoch)

        # ëª¨ë¸ ì €ì¥ (Best model)
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': train_loss,
                'val_loss': val_loss,
            }, 'best_model.pth')
            print(f"   âœ… Best model saved! (Val Loss: {val_loss:.4f})")
            patience_counter = 0
        else:
            patience_counter += 1

        # Early stopping
        if patience_counter >= early_stop_patience:
            print(f"\n   âš ï¸  Early stopping triggered (patience={early_stop_patience})")
            break

    writer.close()
    print()

    # ==================== ê²°ê³¼ ====================
    print("ğŸ“Š Results")
    print("="*50)
    print(f"   Best validation loss: {best_val_loss:.4f}")
    print(f"   Final training loss: {train_losses[-1]:.4f}")
    print(f"   Total epochs: {len(train_losses)}")
    print()

    # í•™ìŠµ ê³¡ì„  ì‹œê°í™”
    plot_training_history(train_losses, val_losses)

    # ==================== ëª¨ë¸ ë¡œë“œ & ì¶”ë¡  ====================
    print("ğŸ”® Inference Example")
    print("="*50)

    # Best model ë¡œë“œ
    checkpoint = torch.load('best_model.pth')
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    # ìƒ˜í”Œ ì¶”ë¡ 
    with torch.no_grad():
        sample_input = X_val[:5].to(device)
        sample_output = model(sample_input)

        print(f"   Input shape: {sample_input.shape}")
        print(f"   Output shape: {sample_output.shape}")
        print(f"   Sample prediction (first 10 dims):")
        print(f"   {sample_output[0, :10].cpu().numpy()}")

    print()
    print("="*50)
    print("âœ… Training completed successfully!")
    print("="*50)
    print("\nğŸ“ Next Steps:")
    print("   1. Visualize with TensorBoard: tensorboard --logdir=runs")
    print("   2. Experiment with hyperparameters")
    print("   3. Try different architectures (CNN, RNN)")
    print("   4. Move to 03_training_loop.py for advanced techniques")
    print()


if __name__ == "__main__":
    # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •
    torch.manual_seed(42)
    np.random.seed(42)

    main()
